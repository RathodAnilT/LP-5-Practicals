{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Practical 4\n",
        "\n",
        "Write a CUDA Program for :\n",
        "1. Addition of two large vectors\n",
        "2. Matrix Multiplication using CUDA C"
      ],
      "metadata": {
        "id": "V-z8LLKHDSfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Vector Addition Using CUDA C"
      ],
      "metadata": {
        "id": "tdlD6sGADreP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SaT8rywCwDp"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pycuda if not already installed\n",
        "!pip install -q pycuda\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "# Define the CUDA kernel for vector addition\n",
        "mod = SourceModule(\"\"\"\n",
        "__global__ void vector_add(int *A, int *B, int *C, int n)\n",
        "{\n",
        "    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    if (idx < n) {\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Function to perform vector addition\n",
        "def vector_add_cuda(A, B):\n",
        "    n = len(A)\n",
        "\n",
        "    # Allocate memory on the device\n",
        "    A_gpu = cuda.mem_alloc(A.nbytes)\n",
        "    B_gpu = cuda.mem_alloc(B.nbytes)\n",
        "    C_gpu = cuda.mem_alloc(A.nbytes)  # Result vector\n",
        "\n",
        "    # Copy data to the device\n",
        "    cuda.memcpy_htod(A_gpu, A)\n",
        "    cuda.memcpy_htod(B_gpu, B)\n",
        "\n",
        "    # Get the CUDA function\n",
        "    vector_add = mod.get_function(\"vector_add\")\n",
        "\n",
        "    # Set up thread block and grid dimensions\n",
        "    threads_per_block = 256\n",
        "    blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "    # Launch the kernel\n",
        "    vector_add(A_gpu, B_gpu, C_gpu, np.int32(n), block=(threads_per_block, 1, 1), grid=(blocks_per_grid, 1))\n",
        "\n",
        "    # Copy result from device to host\n",
        "    C = np.empty_like(A)\n",
        "    cuda.memcpy_dtoh(C, C_gpu)\n",
        "\n",
        "    return C\n",
        "\n",
        "# Function to safely read integer input\n",
        "def safe_input_int(prompt):\n",
        "    while True:\n",
        "        try:\n",
        "            n = int(input(prompt))\n",
        "            if n <= 0:\n",
        "                print(\"⚠️  Size must be positive.\")\n",
        "                continue\n",
        "            return n\n",
        "        except ValueError:\n",
        "            print(\"⚠️  Invalid input. Please enter a single integer.\")\n",
        "\n",
        "# Function to safely read vector elements\n",
        "def safe_input_vector(prompt, n):\n",
        "    while True:\n",
        "        try:\n",
        "            elements = list(map(int, input(prompt).split()))\n",
        "            if len(elements) != n:\n",
        "                print(f\"⚠️  You entered {len(elements)} elements, but {n} expected. Try again.\")\n",
        "                continue\n",
        "            return np.array(elements, dtype=np.int32)\n",
        "        except ValueError:\n",
        "            print(\"⚠️  Invalid input. Please enter integers separated by spaces.\")\n",
        "\n",
        "# Main program\n",
        "n = safe_input_int(\"Enter size of vectors: \")\n",
        "\n",
        "A = safe_input_vector(f\"Enter {n} elements of vector A (space-separated): \", n)\n",
        "B = safe_input_vector(f\"Enter {n} elements of vector B (space-separated): \", n)\n",
        "\n",
        "# Perform vector addition on GPU\n",
        "C = vector_add_cuda(A, B)\n",
        "\n",
        "# Displaying addition steps\n",
        "print(\"\\nAddition Steps:\")\n",
        "for i in range(n):\n",
        "    print(f\"C[{i}] = {A[i]} + {B[i]} = {C[i]}\")\n",
        "\n",
        "# Final Resultant Vector\n",
        "print(\"\\nResultant vector C:\")\n",
        "print(*C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4jfdJInwSWf",
        "outputId": "1e6f0f7e-c27b-47ce-dcf4-24c596bb851e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Enter size of vectors: 2\n",
            "Enter 2 elements of vector A (space-separated): 1 2\n",
            "Enter 2 elements of vector B (space-separated): 3 4\n",
            "\n",
            "Addition Steps:\n",
            "C[0] = 1 + 3 = 4\n",
            "C[1] = 2 + 4 = 6\n",
            "\n",
            "Resultant vector C:\n",
            "4 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Matrix Multiplication using CUDA (PyCUDA in Colab)"
      ],
      "metadata": {
        "id": "3HftXu1XHCj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "# CUDA kernel for matrix multiplication\n",
        "mod = SourceModule(\"\"\"\n",
        "__global__ void matmul(int *A, int *B, int *C, int N)\n",
        "{\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if(row < N && col < N) {\n",
        "        int temp = 0;\n",
        "        for(int i = 0; i < N; i++) {\n",
        "            temp += A[row * N + i] * B[i * N + col];\n",
        "        }\n",
        "        C[row * N + col] = temp;\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Function to perform matrix multiplication on GPU\n",
        "def matrix_multiply_cuda(A, B, N):\n",
        "    # Flatten matrices to 1D arrays\n",
        "    A_flat = A.flatten().astype(np.int32)\n",
        "    B_flat = B.flatten().astype(np.int32)\n",
        "    C_flat = np.zeros_like(A_flat)\n",
        "\n",
        "    # Allocate device memory\n",
        "    A_gpu = cuda.mem_alloc(A_flat.nbytes)\n",
        "    B_gpu = cuda.mem_alloc(B_flat.nbytes)\n",
        "    C_gpu = cuda.mem_alloc(C_flat.nbytes)\n",
        "\n",
        "    # Copy data to device memory\n",
        "    cuda.memcpy_htod(A_gpu, A_flat)\n",
        "    cuda.memcpy_htod(B_gpu, B_flat)\n",
        "\n",
        "    # Get the CUDA function\n",
        "    matmul = mod.get_function(\"matmul\")\n",
        "\n",
        "    # Define grid and block dimensions\n",
        "    block_dim = (16, 16, 1)\n",
        "    grid_dim = (int(np.ceil(N / block_dim[0])), int(np.ceil(N / block_dim[1])), 1)\n",
        "\n",
        "    # Launch the kernel (Matrix multiplication)\n",
        "    matmul(A_gpu, B_gpu, C_gpu, np.int32(N), block=block_dim, grid=grid_dim)\n",
        "\n",
        "    # Copy result from device to host\n",
        "    cuda.memcpy_dtoh(C_flat, C_gpu)\n",
        "\n",
        "    # Reshape result back to a 2D matrix\n",
        "    C = C_flat.reshape((N, N))\n",
        "    return C\n",
        "\n",
        "# Input size and matrices\n",
        "N = int(input(\"Enter the size of the matrices (NxN): \"))\n",
        "print(f\"Enter elements for matrix A ({N}x{N}):\")\n",
        "A = np.array([list(map(int, input().split())) for _ in range(N)], dtype=np.int32)\n",
        "\n",
        "print(f\"Enter elements for matrix B ({N}x{N}):\")\n",
        "B = np.array([list(map(int, input().split())) for _ in range(N)], dtype=np.int32)\n",
        "\n",
        "# Perform matrix multiplication on GPU\n",
        "C = matrix_multiply_cuda(A, B, N)\n",
        "\n",
        "# Display matrices and result\n",
        "print(\"\\nMatrix A:\")\n",
        "print(A)\n",
        "print(\"\\nMatrix B:\")\n",
        "print(B)\n",
        "print(\"\\nResultant Matrix C:\")\n",
        "print(C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiLk1qi_G8Nt",
        "outputId": "7ecad178-130b-47a6-d284-18b535323c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.11/dist-packages (2025.1)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.11/dist-packages (from pycuda) (2025.1.2)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.7)\n",
            "Requirement already satisfied: mako in /usr/lib/python3/dist-packages (from pycuda) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.13.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the size of the matrices (NxN): 2\n",
            "Enter elements for matrix A (2x2):\n",
            "1 2\n",
            "3 4\n",
            "Enter elements for matrix B (2x2):\n",
            "5 6\n",
            "7 8\n",
            "\n",
            "Matrix A:\n",
            "[[1 2]\n",
            " [3 4]]\n",
            "\n",
            "Matrix B:\n",
            "[[5 6]\n",
            " [7 8]]\n",
            "\n",
            "Resultant Matrix C:\n",
            "[[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arPcpbc-HLLX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}